\documentclass[a4paper,10pt]{article}
%\documentclass[a4paper,10pt]{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc} % Polish
\usepackage{url,fullpage,booktabs,amsmath,multicol}
\usepackage[all]{xy}
\usepackage{color,multicol}

\title{The syntax of the 4lang concept lexicon}
\author{Judit Ács, András Kornai, Márton Makrai, Dávid Nemeskey, and Gábor Recski}
\date{2013}

\pdfinfo{%
  /Title    ()
  /Author   ()
  /Creator  ()
  /Producer ()
  /Subject  ()
  /Keywords ()
}

\begin{document}
\maketitle
\section*{Introduction}
    4lang multilingual concept lexicon is hosted at \url{hlt.sztaki.hu/resources/4lang/} and was introduced in \cite{Kornai:2013} in Hungarian. Here, after summarizing its design principles, we specify                        its syntax (Section \ref{sec_synt}) and discuss the defining vocabulary (Section \ref{sec_dv}).

4lang is a multilingual concept lexicon. The original resource contains word forms for concepts in four languages (hence the name), English, Hungarian, Polish, and Latin. For an extension to forty languages see \cite{Acs:2013}.

Entries of 4lang are concepts that are more abstract than words in two
aspects: We aim to capture abstract meanings, so disambiguation is done only
in cases of pure homonymy. On the other hand, 4lang describes conceptual
meaning, so part of speech (POS) differences (\emph{go, going}) are factored
out (as more properly belonging to syntax).
\section{The syntax of 4lang}\label{sec_synt}
4lang is a tab separated file with the following fields. First we list the fields of the file, than describe the definition field.
\subsection{Fields}
\begin{enumerate}
 \item English form
\item Hungarian form
\item Polish form
\item Latin form
\item numerical id
\item membership in defining vocabulary (under construction)
\item POS tag. This field should not be taken too seriously as we have already pointed out that POS is factored out in 4lang. For the an explanation of the abbreviations, see table \ref{table_pos}.
\item definition (see below)
\item comment
\end{enumerate}
Accented characters in word forms (which are common in Hungarian and Polish)
are encoded using Prószéky Codes, see Table \ref{table_proszeky}.

\begin{table}[h]
\begin{center}
\begin{tabular}{rl}
\toprule 
\\\multicolumn{2}{c}{Hungarian}
\\\midrule
\\ a1 & á
\\ e1 & é
\\ i1 & í
\\ o1 & ó
\\ o2 & ö
\\ o3 & ő
\\ u1 & ú
\\ u2 & ü
\\ u3 & ű
\\\bottomrule
\end{tabular}
\begin{tabular}{cc}
\toprule
\\\multicolumn{2}{c}{Polish}
\\\midrule
\\ a1	& ą
\\ c1	& ć
\\ e1	& ę
\\ l1	& ł
\\ n1	& ń
\\ o1	& ó
\\ s1	& ś
\\ z1	& ź
\\ z2	& ż
\\\bottomrule
\end{tabular}
\end{center}
\caption{Prószéky Codes of accented Hungarian and Polish characters}
\label{table_proszeky}
\end{table}
\begin{table}[h]
\begin{center}
\begin{tabular}{rll}
\toprule 
1794	& N & noun
\\ 682	& A & adjective
\\ 590	& V & transitive verb
\\ 156	& G & function word
\\ 151	& U & intransitive verb
\\ 98	& D & adverb
\\ 7	& \# & unknown
\\\bottomrule
\end{tabular}
\end{center}
\caption{POSs in 4lang. The first column contains the number of items with the corresponding POS.}
\label{table_pos}
\end{table}


\subsection{Definitions}
4lang contains two kinds of relations between words. Assertions like \emph{mouse is rodent} or \emph{tail is long} (these are pre-theoretic notations) will be called \emph{predication}\footnote{The only way we encode the linguistic distinction between attribution and prediction proper is that \emph{mouse is rodent} is predicated about the canonical representative of `mouse' (meaning that mice as such are rodents), while \emph{tail is long} is predicated about some instances of `tail' e.g.\ that of the canonical `mouse'.}. In most cases, predication is similar to \texttt{IS\_A} in knowledge representation. The other kind of relation is between \emph{functions} and \emph{arguments}: In \emph{cow makes milk} (represented by the clause \texttt{cow MAKE milk}), \texttt{cow} is the \emph{first argument} of the function \texttt{MAKE}, and \texttt{milk} is its \emph{second} argument. Not only verbs can be predicates. When the predicate is a (transitive) verb, the first argument is usually an agent and a second is a patient. Members of predication (e.g.\ \texttt{mouse} and \texttt{rodent} above), functions (e.g.\ \texttt{MAKE}), and their arguments (e.g.\ \texttt{cow}, \texttt{milk}) are all \emph{concepts}.

Definitions can be interpreted in terms of graphs as well. In graph notation, 4lang is a directed graph whose nodes are labeled with concepts and edges are labeled with $0$, $1$ or $2$. The examples above are represented by $\texttt{mouse}\xrightarrow{0}\texttt{rodent}$ and $\texttt{cow}\xleftarrow{1}\texttt{MAKE}\xrightarrow{2}\texttt{milk}$. So $u\xleftarrow{1}v$ (resp.\ $v\xrightarrow 2 u$) is equivalent to $u$ being the first (resp.\ second) argument of $v$.
A graph representing the whole lexicon is built in two steps: first, every clause in definitions is translated to a graph, and then graphs are connected by \emph{unifying} nodes that have the same label and no outgoing edges.

%We say thatA node representing a single clause is non-function if all its outgoing edges (if any) are labeled with $0$ and function otherwise. Clauses describing subtrees rooted at a (non-)function node will also be called (non-)branching.

\begin{table}[h]
\begin{center}
\begin{tabular}{ll}
 $D$ & definition
\\ $ E $ & expression (clause)
\\ $ E_f $ & expression containing a \emph function
\\ $ E_n  $ & expression containing \emph no function
\\ $ C_f $ & function
\\ $ A $ & argument 
\\ $ C_n $ & a concept that is \emph not a function
\end{tabular}
\end{center}
\caption{Non-terminal symbols}
\label{table_nont}
\end{table}


We give the syntax of the definitions in Table \ref{table_minisynt}. Non-terminals are listed and explained in Table \ref{table_nont} and terminals  in Table \ref{table_termin}. Next to each rewriting rule, we give a graph representing the right side of the production. 
 {\color{red} Relation to TAGs TIGs TSGs? See e.g. Swanson
  et al in ACL13, Yamangil and Shieber ACL13, Hunter and Dyer MOL13}
In the graph part,

\begin{table}[h]
\begin{center}
\begin{tabular}{ll}
 uppercase letters & in function names
\\ lowercase letters & in names of concepts that are not functions
\\ \texttt{,} (comma) & separates clauses
\\ \texttt{[}\ldots \texttt{]} & the formula in the bracket describes the concept before it
\\ \texttt{(}\ldots \texttt{)} & the concept in the parenthesis is characterized by the concept preceding the \\ & parenthesis
\\ \texttt{'} (apostrophe) & fills an argument slot with no conceptual restriction
\\ \texttt{!} & starts deep cases, see Section \ref{sec_extraword}
%\\ \texttt{=root} & see Section \ref{sec_extraword}
\\ \texttt{@} & starts a link to encyclopedia, where non-linguistic knowledge is stored
\\ \texttt{<} \texttt{>}& information in angled brackets are only defaults and are easy to override
\end{tabular}
\end{center}
\caption{Terminal symbols}
\label{table_termin}
\end{table}
 \begin{table}
\begin{center}
{\small
\begin{tabular}{cc}
\\ $ D \rightarrow C^\texttt{*}$ & $\xymatrix{&d\ar^0[dl]\ar^0[dr]\\g(C)&\dots&g(C)}$
\\ $ E \rightarrow E_n \mid E_f $
\\ $ E \rightarrow C_n \texttt{(} E_f \texttt{)}  $ & $\xymatrix{g(E_f)\ar^0[d]\\g(C_n)}$
\\ $ E_f  \rightarrow A C_f $ & $\xymatrix{&g(C_f)\ar^1[dl]\ar^2[dr]\\g(A)&&d}$
\\ $ E_f  \rightarrow C_f A $ & $\xymatrix{&g(C_f)\ar^1[dl]\ar^2[dr]\\d&&g(A)}$
\\ $ E_f  \rightarrow A^{(1)} C_f A^{(2)} $ & $\xymatrix{&g(C_f)\ar^1[dl]\ar^2[dr]\\g(A^{(1)})&&g(A^{(2)})}$
\\ $ E_f  \rightarrow C_f \texttt{'}$ &  $\xymatrix{&g(C_f)\ar^1[dl]\\d}$
\\ $ E_f  \rightarrow \texttt{'}C_f $ & $\xymatrix{g(C_f)\ar^2[dr]\\&d}$
\\ $ E_f  \rightarrow E_n  $% kell ez?
\\ $ E_n   \rightarrow C_n $ 
\\ $ E_n   \rightarrow C_n \texttt{[} D \texttt{]} $ & $\xymatrix{g(C_n)\ar^0[d]\\g(D)}$
\\ $ E_n   \rightarrow C_n^{(1)} \texttt{(} C_n^{(2)} \texttt{)} $ & $\xymatrix{g(C_n^{(2)})\ar^0[d]\\g(C_n^{(1)})}$
\\ $ A  \rightarrow E_n  $ & $\xymatrix{}$
\\ $ A  \rightarrow \texttt{[} D \texttt{]} $ & $\xymatrix{}$
\\ $ C_n  \rightarrow \texttt{!AGT} \mid \texttt{!PAT} \mid \texttt{!POSS}$ \\ $\mid \texttt{!OBL} \mid \texttt{!DAT} \mid \texttt{!FROM} \mid \texttt{!TO}$
\\ $ C_n  \rightarrow \texttt{=root} $ & $\xymatrix{}$
\\ $ C_n  \rightarrow \texttt{@}External\_url $ & $\xymatrix{}$
\\ $ E \rightarrow \texttt{<} C\texttt{>} $ & $\xymatrix{}$
\\ $ A  \rightarrow \texttt{<} A \texttt{>} $ & $\xymatrix{}$
\\ $ C_n  \rightarrow \texttt{<} C_n \texttt{>} $ & $\xymatrix{}$
\\ $ C_n \rightarrow$ \texttt{[a-z-]+(/[0-9]+)}
\\ $ C_f \rightarrow$ \texttt{[A-Z\_]+(/[0-9]+)}
\end{tabular}
}
\end{center}
\caption{The syntax of the definitions}
\label{table_minisynt}
\end{table}

\begin{itemize}
 \item for any nonterminal $X$, $g(X)$ denotes the graph representing the yield of $X$ (which is a single node in the case of $C_f$ and $C_n$),
 \item $d$ denotes a node labeled with the definiendum (the word that is defined),
 \item $X^{(i)}$ is an instance of the nonterminal $X$.
\end{itemize}
Right sides of the first and the last two derivations are regular
expressions. $D$ rewrites to one or more $C$s delimited by comma. $C_n$ and
$C_f$ basically rewrite to a lowercase and an uppercase string
respectively. Hyphen (\texttt{-}) is used in names of affixes
(e.g.\ \texttt{-able}), and underscore (\texttt{\_}) is used in names of some
functions, e.g.\ \texttt{PART\_OF} and multiword phrases (e.g.\ \emph{call up} `telephone'.
 The optional number at the end, separated
by \texttt{/}, serves disambiguation. E.g.\ \texttt{light/739}.  means concept
number 739 (the opposite of `dark(ness)') while \texttt{light/1381} means
concept number 1381, the opposite of `heavy'.

We conclude this chapter by giving an example line of 4lang with the graph representing it:

\begin{verbatim}
mouse	 ege1r	 mus	mysz	 551	 N	 rodent, HAS long(tail)
\end{verbatim}
\xymatrix{&\texttt{mouse}\ar^0[dl]\ar@{->}@/^/^0[dr]\\\texttt{rodent}&&\texttt{HAS}\ar@{->}@/^/^1[ul]\ar^2[dr]\\&&&\texttt{tail}\ar^0[d]\\&&&\texttt{long}}
\subsection{Extra-word nodes}\label{sec_extraword}
Here we describe those nodes of the graph that are outside the level of word
stems: compositional affixation, adposition and deep cases.% The later capture surface arguments of predicates.

4lang contains bound morphemes i.e.\ morphemes that can form a word only
together with some other morpheme. The morpheme in a multi-morpheme word that
has conceptual meaning will be called a \emph{root}, and others will be called
\emph{affixes}. Definitions of affixes specify how the representation of the whole
word has to be built from the representation of the root. The graph
representing the root is denoted by \texttt{!REL} in definitions of
affixes. In definitions of adpositions, the adpositional object is also represented by \texttt{!REL}. The cause for this uniform treatment of roots of affixes and objects of adpositions is that the same semantic relation can be
expressed by an affix in some language and by a free form in some other,
e.g.\ the English word \emph{my} translates to the Hungarian affix
\emph{-m} as in the following example:
\begin{quote}
\begin{tabular}{lll}
    az & (én) & egerem
\\ the & PRON$\langle\text{1stSg}\rangle$ & mouse-POSS$\langle \text{1stSg}\rangle$
\end{tabular} 

`my mouse'
\end{quote}

\emph{Deep cases} serve for building the representation of a verb together with its
arguments (including optional ones). Names and explanation of deep cases are
given in Table \ref{table_deep}.  Agents and patients are relatively
straightforward, relational nouns will be explained later in this
section. Obliques, quirky case marked objects. %TODO
{{Dative means
    the semantic role by \cite{Fillmore:1968}, that Glottopedia describes as
    an animate being that is conscious of being affected by the state or event
    expressed by the verb.}}
 {\color{red} Ez i1gy nagyon rossz terminolo1gia, 
egyse1gesi1teni kell mielo3tt kiadjuk a kezu2nkbo3l.} 

% \subsubsection{Redundancy between deep cases and binary predicates}
% There are cases, when the deep case of an argument is to some extensiont predictable from the formula.
% \begin{table}[h]
% \begin{center}
% \begin{tabular}{rll}
% \toprule 
% 9\%	& $C$ \texttt{HAS} & $C=\texttt{!AGT}$
% \\ 46\%	& \texttt{CAUSE[$C_n1C_fC_n2$]} & $C_n1=\texttt{!PAT}$
% \\ 
% \\\bottomrule
% \end{tabular}
% \end{center}
% \caption{Typical deep cases expressing deep arguments. E.g.\ }
% \label{table_pos}
% \end{table}
% 
%  Some grammar formalisms i.e\. Lexical Mapping Theory in Lexical Functional Grammar \cite{} %TODO
% factor out this redundancy.
% to against defend véd

Now we turn to \emph{relational nouns} e.g.\ \emph{(somebody's) opinion,
  occasion (for something)} that have two semantic arguments: a thinker and
its opinion, an occasion and a goal. We are using underived nouns as examples,
because nominalizations (a rich source of relational nouns) are not
distinguished in 4lang from the verbs they are derived from.  Arguments of
relational nouns are represented by \texttt{!POSS} or \texttt{!OBL}. To give
an insight to the distinction, the remainder of this section comments on
relational nouns in English and Hungarian, languages where most {relational
  nouns} are possessed by their argument.
\begin{quote}
 \begin{tabular}{lll}
  a &fa &töve
 \\the &tree &base$\langle\textsc{POSS}\rangle$
 \end{tabular}
 
 `the base of the tree'
\end{quote}
In these cases we refer to the related word (\emph{tree} in the example) by
\texttt{!POSS}. Note that the possessor in unmarked in Hungarian.
 
About ten percent of the examples behave differently: in Hungarian, the
relational noun is in the sublative (possessive remains an option in some
cases) and in English it is preceded by \emph{for}. Hungarian and English
exceptions do not exactly coincide.
\begin{quote}
 \begin{tabular}{llll}
  kitűnő		& alkalom	& a & mártíromságra
 \\ excellent	& occasion 	& the & martyrdom$\langle\textsc{SUB}\rangle$
 \end{tabular}
 
 `excellent occasion for martyrdom'
\end{quote}
These nouns (\emph{occasion, condition, reason, need}) have in common that the related word is goal of the definiendum in some sense. In these cases we use \texttt{!OBL}.

\begin{table}
\begin{center}
\begin{tabular}{lll}
\\ 454	& \texttt{!AGT}	& agent
\\ 356	& \texttt{!PAT}	& patient
\\ 65	& \texttt{!POSS}& used with some relational nouns
\\ 33	& \texttt{!DAT}	& dative, see the explanation in the text
\\ 32	& \texttt{!REL} & root or adpositional object
\\ 16	& \texttt{!TO}	& target of action
\\ 14	& \texttt{!FROM}& source of action
\\ 6	& \texttt{!OBL} & oblique, quirky case marked argument
\\ 3	& \texttt{!AT}	& location of action
\end{tabular}

\end{center}
\caption{Deep cases}
\label{table_deep}
\end{table}
\section{The defining vocabulary}\label{sec_dv}% TODO goal and overview of the section
Dictionaries aim at defining words with simpler ones. Some monolingual
dictionaries use a defining vocabulary (DV) for this purpose. In definitions,
only words belonging to the DV are allowed, or at least every word has to be
reducible to the DV by iteratively replacing non-DV words with their
definitions. We will call any set of words DV if this criterion holds.  If the
reader, or the computer, knows the words in the DV, they can understand every
word that has a headword in the dictionary.  We note that a list of word forms
is not enough: the lexicographer has to specify which sense of a word can be
used in the DV, and definitions should not include set phrases (unless the set
phrase as a whole is listed in the DV). The smaller the DV the better. In this
section we investigate how a small DV can be computed. We propose two methods.

The first method is based on the concept that words appearing more frequently
in definitions are more basic in some intuitive sense. We take as a starting
point the set of all words appearing in any definition, and eliminate words by
replacing them with all the words in their definitions. We forbid eliminations
where the definition contains a word that was eliminated earlier, so the
process will terminate. When no more words can be eliminated, the resulting
set is a DV that is minimal with respect to inclusion. The result depends on
the order of the word eliminations. We used the intuition that frequent
defining words are more basic, and start elimination with the least frequent
words and progress in ascending order.

The other approach looks at the dictionary as a directed graph: nodes are
words (headwords and defining words as well), and there is an edge from the
node representing \emph{steel} to \emph{metal} if \emph{metal} appears in the
definition of \emph{steel}. Two nodes are \emph{strongly connected} if there
is a path from them to each other. Strong connectivity is an equivalence
relation whose classes are called \emph{strongly connected components}. 

We
computed the strongly connected components of the Longman Dictionary of
Contemporary English(LDOCE). (The original version of LDOCE was created using a DV as we describe here \cite{Boguraev:1989}. The version we have access to is \cite{Bullon:2003}.)
In the remainder of this paragraph we report how word forms in the definitions were mapped to defined words (headwords). We checked the following modifications of the word form in this order to find the corresponding headword:
%preprocessing of definitions after which there are 50624
\begin{enumerate}
 \item Named entities and words containing non-letters have been discarded.
 \item the string obtained by removing puctuation (Python's string.punctuation except for hyphen and apostrophe)
 \item the string obtained by removing apostrophes expect for those preceeding \emph{s, t, ll, m, re} or \emph{nt} e.g.\emph{John\textbf{'}s}
 \item stem of the word provided by ocamorph \cite{ocamorph} or hand mapping
 \item the string obtained by optionally removing every hyphen from the beginning or the end
 \item optionally removing or replacing with space every string-internal hyphen
\end{enumerate}

After preprocessing, we have 50624
nodes. A histogram of component sizes can be found in table
\ref{table_compon}. The exact numbers depend on the subtleties of
preprocessing (e.g.\ treatment of non-alphanumeric characters and named
entities, stemming (inflection and derivation as well)), but the picture is
robust.


\begin{table}
 \begin{center}
  \begin{tabular}{rr}
   \toprule
   \# of comps & comp size
   \\\midrule
   \\  47161	& 1
   \\  24	& 2
   \\  3	& 3
   \\  1        & 3406
   \\ \bottomrule
  \end{tabular}
  \caption{Sizes of strongly connected components in Longman}
  \label{table_compon}
 \end{center}
\end{table}

A great majority (93\%) of the words are singletons as they are not used in
any definition. Again a great majority (98\%) of the remainder form a great
component, and there are some components with two or three elements. The great
component (possibly together with the 2--3 element ones that do not
significantly increase size) is suitable as a DV that is not necessarily
minimal. Two kind of reductions are possible, one is based on breaking cycles,
the other is finding a variant of vertex cover. Given a directed graph with
vertices $V$, we will say that a node $v\in V$ is covered by a set of nodes
$C\subseteq V$ iff either $v\in C$ or all the out-neighbors of $v$ are covered
by $C$ (the definition is recursive). We call $C$ a \emph{strong vertex cover}
iff it covers all nodes in $V$. {\color{red} Ez a vertex cover me1g csak az
  e1tva1gy felkelte1se1hez elegendo3, itt sok minden kell me1g.}
%TODO megkeresni az összes kört
% Some cycles consist of synonyms (\emph{badminton -- shuttlecock}) or stem and affixed form (
\section*{Acknowledgement}
Judit Ács preprocessing of Longman,
András Kornai advision,
Márton Makrai,
Dávid Nemeskey design of definition syntax,
Gábor Recski preprocessing of Longman
\bibliographystyle{apalike}
\bibliography{ml}
\end{document}
